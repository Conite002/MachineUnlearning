{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ../install-venv.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./../install-venv.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f89d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.7.0 (from -r ../requirements.txt (line 1))\n",
      "  Using cached tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
      "Collecting scikit-learn==1.0.2 (from -r ../requirements.txt (line 2))\n",
      "  Using cached scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting nltk==3.8.1 (from -r ../requirements.txt (line 3))\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting tqdm==4.65.0 (from -r ../requirements.txt (line 4))\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)\n",
      "Collecting click==8.1.3 (from -r ../requirements.txt (line 5))\n",
      "  Using cached click-8.1.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting matplotlib==3.5.3 (from -r ../requirements.txt (line 6))\n",
      "  Using cached matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting pandas==1.3.5 (from -r ../requirements.txt (line 7))\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting seaborn==0.12.2 (from -r ../requirements.txt (line 8))\n",
      "  Using cached seaborn-0.12.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting numpy>=1.14.5 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting absl-py>=0.4.0 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting libclang>=9.0.1 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting flatbuffers<3.0,>=1.12 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached flatbuffers-2.0.7-py2.py3-none-any.whl.metadata (872 bytes)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting protobuf>=3.9.2 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ../requirements.txt (line 1)) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from tensorflow==2.7.0->-r ../requirements.txt (line 1)) (4.7.1)\n",
      "Collecting wheel<1.0,>=0.32.0 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached wheel-0.42.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached wrapt-1.16.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting gast<0.5.0,>=0.2.1 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting tensorboard~=2.6 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting tensorflow-estimator<2.8,~=2.7.0rc0 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached tensorflow_estimator-2.7.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting keras<2.8,>=2.7.0rc0 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached keras-2.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.21.0 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.34.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached grpcio-1.62.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting scipy>=1.1.0 (from scikit-learn==1.0.2->-r ../requirements.txt (line 2))\n",
      "  Using cached scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting joblib>=0.11 (from scikit-learn==1.0.2->-r ../requirements.txt (line 2))\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.0.2->-r ../requirements.txt (line 2))\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk==3.8.1->-r ../requirements.txt (line 3))\n",
      "  Using cached regex-2024.4.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from click==8.1.3->-r ../requirements.txt (line 5)) (6.7.0)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.5.3->-r ../requirements.txt (line 6))\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.5.3->-r ../requirements.txt (line 6))\n",
      "  Using cached fonttools-4.38.0-py3-none-any.whl.metadata (138 kB)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib==3.5.3->-r ../requirements.txt (line 6))\n",
      "  Using cached kiwisolver-1.4.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from matplotlib==3.5.3->-r ../requirements.txt (line 6)) (24.0)\n",
      "Collecting pillow>=6.2.0 (from matplotlib==3.5.3->-r ../requirements.txt (line 6))\n",
      "  Using cached Pillow-9.5.0-cp37-cp37m-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting pyparsing>=2.2.1 (from matplotlib==3.5.3->-r ../requirements.txt (line 6))\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from matplotlib==3.5.3->-r ../requirements.txt (line 6)) (2.9.0.post0)\n",
      "Collecting pytz>=2017.3 (from pandas==1.3.5->-r ../requirements.txt (line 7))\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting protobuf>=3.9.2 (from tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (679 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1)) (41.2.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached Werkzeug-2.2.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from importlib-metadata->click==8.1.3->-r ../requirements.txt (line 5)) (3.15.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1)) (3.7)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1)) (2.1.5)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r ../requirements.txt (line 1))\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Using cached tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n",
      "Using cached scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Using cached matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Using cached flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
      "Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.62.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "Using cached h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Using cached kiwisolver-1.4.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached Pillow-9.5.0-cp37-cp37m-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached regex-2024.4.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (761 kB)\n",
      "Using cached scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Using cached protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Using cached tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.34.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Using cached wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "Using cached wrapt-1.16.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "Using cached google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Using cached Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
      "Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard-plugin-wit, pytz, libclang, keras, flatbuffers, wrapt, wheel, werkzeug, urllib3, tqdm, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, regex, pyparsing, pyasn1, protobuf, pillow, oauthlib, numpy, kiwisolver, joblib, grpcio, google-pasta, gast, fonttools, cycler, charset-normalizer, certifi, cachetools, absl-py, scipy, rsa, requests, pyasn1-modules, pandas, opt-einsum, matplotlib, markdown, keras-preprocessing, h5py, click, astunparse, seaborn, scikit-learn, requests-oauthlib, nltk, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.3 cycler-0.11.0 flatbuffers-2.0.7 fonttools-4.38.0 gast-0.4.0 google-auth-2.29.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.62.2 h5py-3.8.0 joblib-1.3.2 keras-2.7.0 keras-preprocessing-1.1.2 kiwisolver-1.4.5 libclang-18.1.1 markdown-3.4.4 matplotlib-3.5.3 nltk-3.8.1 numpy-1.21.6 oauthlib-3.2.2 opt-einsum-3.3.0 pandas-1.3.5 pillow-9.5.0 protobuf-3.20.3 pyasn1-0.5.1 pyasn1-modules-0.3.0 pyparsing-3.1.2 pytz-2024.1 regex-2024.4.16 requests-2.31.0 requests-oauthlib-2.0.0 rsa-4.9 scikit-learn-1.0.2 scipy-1.7.3 seaborn-0.12.2 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.3.0 threadpoolctl-3.1.0 tqdm-4.65.0 urllib3-2.0.7 werkzeug-2.2.3 wheel-0.42.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r  ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df90385a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel==6.16.2 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from -r ../opt_requirements.txt (line 1)) (6.16.2)\n",
      "Collecting tensorflow-datasets==4.8.2 (from -r ../opt_requirements.txt (line 2))\n",
      "  Downloading tensorflow_datasets-4.8.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: debugpy>=1.0 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (1.7.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (7.34.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (7.4.9)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: packaging in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (24.0)\n",
      "Requirement already satisfied: psutil in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (26.0.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (6.2)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (5.9.0)\n",
      "Requirement already satisfied: absl-py in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: click in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (8.1.3)\n",
      "Collecting dill (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2))\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting dm-tree (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2))\n",
      "  Downloading dm_tree-0.1.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting etils>=0.9.0 (from etils[enp,epath]>=0.9.0->tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2))\n",
      "  Downloading etils-0.9.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (1.21.6)\n",
      "Collecting promise (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2))\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (3.20.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (2.31.0)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2))\n",
      "  Downloading tensorflow_metadata-1.12.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: termcolor in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (2.3.0)\n",
      "Collecting toml (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2))\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: wrapt in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (4.7.1)\n",
      "Requirement already satisfied: importlib-resources in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (5.12.0)\n",
      "Requirement already satisfied: zipp in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (3.15.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (41.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (0.19.1)\n",
      "Requirement already satisfied: decorator in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (3.0.43)\n",
      "Requirement already satisfied: pygments in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (2.17.2)\n",
      "Requirement already satisfied: backcall in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from ipython>=7.23.1->ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: entrypoints in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (4.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from click->tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (6.7.0)\n",
      "Requirement already satisfied: six in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from promise->tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2)) (1.16.0)\n",
      "Collecting absl-py (from tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2))\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata->tensorflow-datasets==4.8.2->-r ../opt_requirements.txt (line 2))\n",
      "  Using cached googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel==6.16.2->-r ../opt_requirements.txt (line 1)) (0.2.13)\n",
      "Downloading tensorflow_datasets-4.8.2-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading etils-0.9.0-py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dm_tree-0.1.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.8/153.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_metadata-1.12.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
      "Installing collected packages: dm-tree, toml, promise, googleapis-common-protos, etils, dill, absl-py, tensorflow-metadata, tensorflow-datasets\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 2.1.0\n",
      "    Uninstalling absl-py-2.1.0:\n",
      "      Successfully uninstalled absl-py-2.1.0\n",
      "Successfully installed absl-py-1.4.0 dill-0.3.7 dm-tree-0.1.8 etils-0.9.0 googleapis-common-protos-1.63.0 promise-2.3 tensorflow-datasets-4.8.2 tensorflow-metadata-1.12.0 toml-0.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r  ../opt_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5864f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 23:35:37.156366: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-05-26 23:35:37.156401: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# Only if you are using CUDA devices\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Import necessary modules\n",
    "from conf import BASE_DIR\n",
    "from Applications.Poisoning.gen_configs import main as gen_configs\n",
    "from Applications.Poisoning.train import main as train\n",
    "from Applications.Poisoning.poison.poison_models import train_poisoned\n",
    "from Applications.Poisoning.configs.demo.config import Config\n",
    "from Applications.Poisoning.unlearn.first_order import run_experiment as fo_experiment\n",
    "from Applications.Poisoning.unlearn.second_order import run_experiment as so_experiment\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63950d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d2fab",
   "metadata": {},
   "source": [
    "# Set figure size for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12423123",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 11, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a26b45",
   "metadata": {},
   "source": [
    "# Download the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "400014b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: cifar10\n",
      "Shapes of data:\n",
      "  - Training data: (50000, 32, 32, 3), (50000, 10)\n",
      "  - Validation data: (5000, 32, 32, 3), (5000, 10)\n",
      "  - Test data: (5000, 32, 32, 3), (5000, 10)\n",
      "Finished processing dataset: cifar10\n",
      "\n",
      "Processing dataset: mnist\n",
      "Shapes of data:\n",
      "  - Training data: (60000, 28, 28, 1), (60000, 10)\n",
      "  - Validation data: (5000, 28, 28, 1), (5000, 10)\n",
      "  - Test data: (5000, 28, 28, 1), (5000, 10)\n",
      "Finished processing dataset: mnist\n",
      "\n",
      "Processing dataset: fashion_mnist\n",
      "Shapes of data:\n",
      "  - Training data: (60000, 28, 28, 1), (60000, 10)\n",
      "  - Validation data: (5000, 28, 28, 1), (5000, 10)\n",
      "  - Test data: (5000, 28, 28, 1), (5000, 10)\n",
      "Finished processing dataset: fashion_mnist\n",
      "\n",
      "Processing dataset: cifar100\n",
      "Shapes of data:\n",
      "  - Training data: (50000, 32, 32, 3), (50000, 100)\n",
      "  - Validation data: (5000, 32, 32, 3), (5000, 100)\n",
      "  - Test data: (5000, 32, 32, 3), (5000, 100)\n",
      "Finished processing dataset: cifar100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from conf import BASE_DIR\n",
    "\n",
    "datasets_to_test = ['cifar10', 'mnist', 'fashion_mnist', 'cifar100']\n",
    "data_dir = '../.data'\n",
    "\n",
    "for dataset_name in datasets_to_test:\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    \n",
    "    dataset = tfds.load(dataset_name, data_dir=data_dir)\n",
    "    \n",
    "    x_train, y_train = list(zip(*((sample['image'], sample['label']) for sample in dataset['train'])))\n",
    "    x_train = np.stack(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    x_test, y_test = list(zip(*((sample['image'], sample['label']) for sample in dataset['test'])))\n",
    "    x_test = np.stack(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Handle the case where number of classes is different\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    \n",
    "    y_train = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=n_classes)\n",
    "\n",
    "    x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42)\n",
    "    y_val = to_categorical(y_val.argmax(axis=1), num_classes=n_classes)\n",
    "\n",
    "    dataset_dir = BASE_DIR/'train_test_data'/dataset_name\n",
    "    dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Print information about the dataset\n",
    "    print(f\"Shapes of data:\")\n",
    "    print(f\"  - Training data: {x_train.shape}, {y_train.shape}\")\n",
    "    print(f\"  - Validation data: {x_val.shape}, {y_val.shape}\")\n",
    "    print(f\"  - Test data: {x_test.shape}, {y_test.shape}\")\n",
    "\n",
    "    for arr, filename in [\n",
    "            (x_train, 'x_train.npy'),\n",
    "            (y_train, 'y_train.npy'),\n",
    "            (x_test, 'x_test.npy'),\n",
    "            (y_test, 'y_test.npy'),\n",
    "            (x_val, 'x_valid.npy'),\n",
    "            (y_val, 'y_valid.npy')]:\n",
    "        np.save(dataset_dir/filename, arr)\n",
    "\n",
    "    print(f\"Finished processing dataset: {dataset_name}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "942f062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets_to_test = ['cifar10', 'mnist', 'fashion_mnist', 'cifar100']\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d16548c",
   "metadata": {},
   "source": [
    "# Paths for configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "599e231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = BASE_DIR / 'models' / 'poisoning'\n",
    "train_conf = BASE_DIR / 'Applications' / 'Poisoning' / 'configs' / 'demo' / 'train.json'\n",
    "poison_conf = BASE_DIR / 'Applications' / 'Poisoning' / 'configs' / 'demo' / 'poison.json'\n",
    "unlearn_conf = BASE_DIR / 'Applications' / 'Poisoning' / 'configs' / 'demo' / 'unlearn.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640df51a",
   "metadata": {},
   "source": [
    "# Generate configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c4e7f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_configs(model_folder, train_conf, poison_conf, unlearn_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acd96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2d2005c",
   "metadata": {},
   "source": [
    "# Define a function to evaluate model accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5743c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_folder, x_test, y_test):\n",
    "    # Dummy implementation; replace with actual evaluation logic\n",
    "    accuracy = np.random.uniform(0.75, 0.95)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946473aa",
   "metadata": {},
   "source": [
    "# Define a function to process a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e41f3490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_name):\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = tfds.load(dataset_name, data_dir='../.data')\n",
    "    \n",
    "    # Preprocess data\n",
    "    x_train, y_train = list(zip(*((sample['image'], sample['label']) for sample in dataset['train'])))\n",
    "    x_train = np.stack(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    x_test, y_test = list(zip(*((sample['image'], sample['label']) for sample in dataset['test'])))\n",
    "    x_test = np.stack(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Handle the case where number of classes is different\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    y_train = to_categorical(y_train, num_classes=n_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=n_classes)\n",
    "    y_val = to_categorical(y_val, num_classes=n_classes)\n",
    "\n",
    "    # Train clean model\n",
    "    clean_model_folder = BASE_DIR / 'models' / 'poisoning' / 'clean'\n",
    "    train(model_folder=clean_model_folder, data=dataset_name)\n",
    "    \n",
    "    # Evaluate clean model\n",
    "    accuracy_clean = evaluate_model(clean_model_folder, x_test, y_test)\n",
    "\n",
    "    # Train poisoned model\n",
    "    poisoned_folder = BASE_DIR / 'models' / 'poisoning' / 'budget-10000' / 'seed-42'\n",
    "    poison_kwargs = Config.from_json(poisoned_folder / 'poison_config.json')\n",
    "    train_kwargs = Config.from_json(poisoned_folder / 'train_config.json')\n",
    "    train_poisoned(model_folder=poisoned_folder, poison_kwargs=poison_kwargs, train_kwargs=train_kwargs)\n",
    "    \n",
    "    # Evaluate poisoned model\n",
    "    accuracy_poisoned = evaluate_model(poisoned_folder, x_test, y_test)\n",
    "\n",
    "    # First-order unlearning\n",
    "    fo_unlearn_kwargs = Config.from_json(poisoned_folder / 'first-order' / 'unlearn_config.json')\n",
    "    fo_experiment(poisoned_folder / 'first-order', train_kwargs, poison_kwargs, fo_unlearn_kwargs)\n",
    "    accuracy_fo_unlearned = evaluate_model(poisoned_folder / 'first-order', x_test, y_test)\n",
    "\n",
    "    # Second-order unlearning\n",
    "    so_unlearn_kwargs = Config.from_json(poisoned_folder / 'second-order' / 'unlearn_config.json')\n",
    "    so_experiment(poisoned_folder / 'second-order', train_kwargs, poison_kwargs, so_unlearn_kwargs)\n",
    "    accuracy_so_unlearned = evaluate_model(poisoned_folder / 'second-order', x_test, y_test)\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        'accuracy_clean': accuracy_clean,\n",
    "        'accuracy_poisoned': accuracy_poisoned,\n",
    "        'accuracy_fo_unlearned': accuracy_fo_unlearned,\n",
    "        'accuracy_so_unlearned': accuracy_so_unlearned\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419593df",
   "metadata": {},
   "source": [
    "# Process each dataset and collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0042c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: cifar10\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 128)       3584      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 128)      512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,800,842\n",
      "Trainable params: 1,798,282\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:step:0,0.0,0.7382000088691711,10001,7962,2039\n",
      "INFO:batch:1,1,0.0\n",
      "INFO:step:1,0.0,0.7418000102043152,10001,7962,2039\n",
      "INFO:batch:2,1,0.001953125\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets_to_test:\n",
    "    results[dataset_name] = process_dataset(dataset_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49031f",
   "metadata": {},
   "source": [
    "# Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, result in results.items():\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Accuracy on clean data: {result['accuracy_clean']}\")\n",
    "    print(f\"Accuracy on poisoned data: {result['accuracy_poisoned']}\")\n",
    "    print(f\"Accuracy after first-order unlearning: {result['accuracy_fo_unlearned']}\")\n",
    "    print(f\"Accuracy after second-order unlearning: {result['accuracy_so_unlearned']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a347959",
   "metadata": {},
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea966221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "datasets = list(results.keys())\n",
    "accuracy_clean = [results[ds]['accuracy_clean'] for ds in datasets]\n",
    "accuracy_poisoned = [results[ds]['accuracy_poisoned'] for ds in datasets]\n",
    "accuracy_fo_unlearned = [results[ds]['accuracy_fo_unlearned'] for ds in datasets]\n",
    "accuracy_so_unlearned = [results[ds]['accuracy_so_unlearned'] for ds in datasets]\n",
    "\n",
    "x = np.arange(len(datasets))  # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width*1.5, accuracy_clean, width, label='Clean Data')\n",
    "rects2 = ax.bar(x - width/2, accuracy_poisoned, width, label='Poisoned Data')\n",
    "rects3 = ax.bar(x + width/2, accuracy_fo_unlearned, width, label='First-Order Unlearned')\n",
    "rects4 = ax.bar(x + width*1.5, accuracy_so_unlearned, width, label='Second-Order Unlearned')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy comparison by dataset')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(datasets)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
